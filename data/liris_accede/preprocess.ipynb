{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "## Global flags\n",
    "\n",
    "# Set to True to remove clips with unusually long or short durations. Is expected to remove 20 clips.\n",
    "REMOVE_OUTLIERS = True\n",
    "\n",
    "# How to select the frames that represent each video. Set to True to select random frames. Set to False to select frames at evenly spaced intervals.\n",
    "# Recommended: False\n",
    "SELECT_FRAMES_RANDOMLY = False\n",
    "\n",
    "# Set to True to delete audio proxies after preprocessing to free up disk space. \n",
    "# Recommended: True\n",
    "DELETE_PROXIES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import os\n",
    "from scipy.io.wavfile import read as wavread\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_accede_scores = pd.read_csv(\"annotations/ACCEDEaffect.txt\", sep=\"\\t\")\n",
    "df_mediaeval_scores = pd.read_csv(\"annotations/MEDIAEVALaffect.txt\", sep=\"\\t\")\n",
    "\n",
    "df_accede_sets = pd.read_csv(\"annotations/ACCEDEsets.txt\", sep=\"\\t\")\n",
    "df_mediaeval_sets = pd.read_csv(\"annotations/MEDIAEVALsets.txt\", sep=\"\\t\")\n",
    "\n",
    "df_scores = pd.concat([df_accede_scores.assign(source='ACCEDE'), df_mediaeval_scores.assign(source='MEDIAEVAL')], ignore_index=True) # Contains valence and arousal classes\n",
    "df_sets = pd.concat([df_accede_sets, df_mediaeval_sets], ignore_index=True) # Contains data split\n",
    "\n",
    "# Join dataframes\n",
    "df_joined = pd.merge(df_scores, df_sets, left_index=True, right_index=True, suffixes=('_scores', '_sets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "display(df_joined)\n",
    "print(df_joined.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "\n",
    "### Removes clips with unusually long or short durations. Is expected to remove 20 outliers from the extended LIRIS-ACCEDE dataset.\n",
    "\n",
    "Cell execution may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if REMOVE_OUTLIERS:\n",
    "    def get_clip_duration(file_id):\n",
    "            video_load_path = 'data/'+file_id+'.mp4'\n",
    "            video = cv2.VideoCapture(video_load_path)\n",
    "            frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "            duration = frame_count / video.get(cv2.CAP_PROP_FPS)\n",
    "            return duration\n",
    "        \n",
    "    # Add clip durations to dataframe\n",
    "    df_joined['duration'] = df_joined['name_scores'].apply(get_clip_duration)\n",
    "\n",
    "    # Calculate clip duration IQR\n",
    "    clip_durations_q1 = np.percentile(df_joined['duration'], 25, interpolation='midpoint')\n",
    "    clip_durations_q3 = np.percentile(df_joined['duration'], 75, interpolation='midpoint')\n",
    "    clip_durations_iqr = clip_durations_q3 - clip_durations_q1\n",
    "\n",
    "    # Calculate inner bounds\n",
    "    clip_durations_lower_bound = clip_durations_q1 - 1.5 * clip_durations_iqr\n",
    "    clip_durations_upper_bound = clip_durations_q3 + 1.5 * clip_durations_iqr\n",
    "\n",
    "    print(f'Lower bound: {clip_durations_lower_bound:.3f} seconds')\n",
    "    print(f'Upper bound: {clip_durations_upper_bound:.3f} seconds')\n",
    "\n",
    "    # Remove outliers\n",
    "    num_rows_before = len(df_joined)\n",
    "\n",
    "    df_joined = df_joined[(df_joined['duration'] >= clip_durations_lower_bound) & (df_joined['duration'] <= clip_durations_upper_bound)]\n",
    "    df_joined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    num_rows_after = len(df_joined)\n",
    "\n",
    "    print(f'Removed {num_rows_before - num_rows_after} outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_width = 384\n",
    "down_height = 224\n",
    "down_points = (down_width, down_height)\n",
    "\n",
    "num_frames = 16 # Number of selected frames\n",
    "\n",
    "num_rows = len(df_joined.index)\n",
    "\n",
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    video_load_path = 'data/'+file_id+'.mp4'\n",
    "    cam = cv2.VideoCapture(video_load_path)\n",
    "    total_frames = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if SELECT_FRAMES_RANDOMLY:\n",
    "        frame_idxs = sorted(random.sample(range(total_frames), num_frames))\n",
    "    else:\n",
    "        frame_idxs = np.linspace(0, (total_frames-1), num=num_frames).round().astype('int').tolist()\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for idx, frame_idx in enumerate(frame_idxs):\n",
    "        \n",
    "        cam.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cam.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('Warning: frame not properly read')\n",
    "            continue\n",
    "        else:\n",
    "            resized_frame = cv2.resize(frame, down_points, interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(resized_frame)\n",
    "\n",
    "    video = np.array(frames)\n",
    "    tensor_video = torch.from_numpy(video)\n",
    "\n",
    "    torch.save(tensor_video, 'preprocessed/videos/' + file_id + '.pt')\n",
    "\n",
    "print('Videos preprocessed and saved to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(df_joined.index)\n",
    "\n",
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "\n",
    "    # Create 9 classes from 3 valence and 3 arousal classes\n",
    "    valenceClass = row['valenceClass']\n",
    "    arousalClass = row['arousalClass']\n",
    "\n",
    "    if(valenceClass == -1):\n",
    "        if(arousalClass == -1): vaClass = 0 # negative calm\n",
    "        elif(arousalClass == 0): vaClass = 1 # negative neutral\n",
    "        elif(arousalClass == 1): vaClass = 2 # negative active\n",
    "        else: raise ValueError('Illegal arousal label value')\n",
    "    elif(valenceClass == 0):\n",
    "        if(arousalClass == -1): vaClass = 3 # neutral calm\n",
    "        elif(arousalClass == 0): vaClass = 4 # both neutral\n",
    "        elif(arousalClass == 1): vaClass = 5 # neutral active\n",
    "        else: raise ValueError('Illegal arousal label value')\n",
    "    elif(valenceClass == 1):\n",
    "        if(arousalClass == -1): vaClass = 6 # positive calm\n",
    "        elif(arousalClass == 0): vaClass = 7 # positive neutral\n",
    "        elif(arousalClass == 1): vaClass = 8 # positive active\n",
    "        else: raise ValueError('Illegal arousal label value')\n",
    "    else: raise ValueError('Illegal valence label value')\n",
    "\n",
    "    torch.save(vaClass, 'preprocessed/labels/' + file_id + '.pt')\n",
    "\n",
    "print('Labels preprocessed and saved to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create proxy audio files (to speed up processing) and get duration of longest audio (for padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "\n",
    "num_rows = len(df_joined.index)\n",
    "\n",
    "# Create audio proxies and get longest audio\n",
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    video_load_path = 'data/'+file_id+'.mp4'\n",
    "    audio_proxy_path = 'audio_proxies/'+file_id+'.wav'\n",
    "\n",
    "    # Create proxy .wav files to speed up audio loading\n",
    "    if not os.path.exists(audio_proxy_path):\n",
    "        command = \"ffmpeg -i \" + video_load_path + \" -ab 160k -ac 1 -ar 48000 -vn \" + audio_proxy_path\n",
    "        subprocess.run(command, shell=True, capture_output=True, text=True, input='y')\n",
    "    \n",
    "    sr, audio = wavread(audio_proxy_path)\n",
    "    audios.append(audio)\n",
    "\n",
    "# Get the duration of the longest audio clip\n",
    "highest_duration = 0\n",
    "for audio in audios:\n",
    "    if len(audio) > highest_duration:\n",
    "        highest_duration = len(audio)\n",
    "max_duration = highest_duration\n",
    "print(f'Maximum duration: {max_duration} data points, or {(max_duration/48_000):.3f} seconds')\n",
    "\n",
    "del audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad audio and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    audio_proxy_path = 'audio_proxies/'+file_id+'.wav'\n",
    "    sr, audio = wavread(audio_proxy_path)\n",
    "    \n",
    "    if sr != 48_000:\n",
    "        print('Warning: Audio ' + file_id + ' has sample rate of ' + sr + ' but requires 48000')\n",
    "        \n",
    "    # Pad audio so that all audios have same duration (necessary for transformation to tensors)\n",
    "    audio_padded = np.zeros((max_duration))\n",
    "    for i in range(0, len(audio)-1):\n",
    "        audio_padded[i] = audio[i]\n",
    "\n",
    "    torch.save(audio_padded, 'preprocessed/audios/' + file_id + '.pt')\n",
    "    \n",
    "print('Audio data saved to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete proxy audio files\n",
    "\n",
    "Remove proxy files to free up disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove audio proxies\n",
    "if DELETE_PROXIES:\n",
    "    for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "        file_id = row['name_scores']\n",
    "        audio_proxy_path = 'audio_proxies/'+file_id+'.wav'\n",
    "        os.remove(audio_proxy_path)\n",
    "    print('Deleted audio proxies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended LIRIS-ACCEDE data split\n",
    "\n",
    "Data split as described in ACCEDEsets.txt and MEDIAEVALsets.txt, resulting in a ratio of 22.5% train, 22.5% val, and 55% test. Does **not** correspond to any dataset described in the thesis.\n",
    "\n",
    "**Note: This data split DOES include the additional instances added in the MediaEval 2015 Affective Impact of Movies Task.**\n",
    "\n",
    "Use the following parameter to use this data split when calling main.py: ```--dataset holdout```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "num_rows = len(df_joined.index)\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "val_ids = []\n",
    "\n",
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    set = row['set']\n",
    "    \n",
    "    if set == 1: # train\n",
    "        train_ids.append(file_id)\n",
    "    elif set == 0: # test\n",
    "        test_ids.append(file_id)\n",
    "    elif set == 2: # val\n",
    "        val_ids.append(file_id)\n",
    "\n",
    "torch.save(train_ids, 'preprocessed/data_splits/train_ids_holdout.pt')\n",
    "torch.save(test_ids, 'preprocessed/data_splits/test_ids_holdout.pt')\n",
    "torch.save(val_ids, 'preprocessed/data_splits/val_ids_holdout.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original LIRIS-ACCEDE data split\n",
    "\n",
    "Data split as described in ACCEDEsets.txt only, resulting in a ratio of 25% train, 25% val, and 50% test. Corresponds to the **LIRIS-ACCEDE with AIMT15 labels** dataset described in the thesis.\n",
    "\n",
    "**Note: This data split does NOT include the additional instances added in the MediaEval 2015 Affective Impact of Movies Task.**\n",
    "\n",
    "Use the following parameter to use this data split when calling main.py: ```--dataset holdout_accede```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ACCEDE only\n",
    "df_accede_joined = df_joined.loc[df_joined['source'] == 'ACCEDE']\n",
    "\n",
    "num_rows = len(df_accede_joined.index)\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "val_ids = []\n",
    "\n",
    "for index, row in tqdm(df_accede_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    set = row['set']\n",
    "    \n",
    "    if set == 1: # train\n",
    "        train_ids.append(file_id)\n",
    "    elif set == 0: # test\n",
    "        test_ids.append(file_id)\n",
    "    elif set == 2: # val\n",
    "        val_ids.append(file_id)\n",
    "\n",
    "torch.save(train_ids, 'preprocessed/data_splits/train_ids_holdout_accede.pt')\n",
    "torch.save(test_ids, 'preprocessed/data_splits/test_ids_holdout_accede.pt')\n",
    "torch.save(val_ids, 'preprocessed/data_splits/val_ids_holdout_accede.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIMT15 data split\n",
    "\n",
    "Data split as described in shots-devset-nl.txt and shots-testset-nl.txt, resulting in a devset(56%) and a testset(44%). Corresponds to the **AIMT15** dataset described in the thesis.\n",
    "\n",
    "The training and validation split for the devset was not specified in MediaEval 2015. We chose a stratified 80/20 split. In total, the resulting ratio is 45% train, 11% val, and 44% test.\n",
    "\n",
    "Use the following parameter to use this data split when calling main.py: ```--dataset mediaeval```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_devset_nl = pd.read_csv(\"annotations/shots-devset-nl.txt\", delimiter='\\t', header=None, names=['mediaeval_id', 'name_sets'])\n",
    "df_testset_nl = pd.read_csv(\"annotations/shots-testset-nl.txt\", delimiter='\\t', header=None, names=['mediaeval_id', 'name_sets'])\n",
    "\n",
    "df_mediaeval_sets = pd.concat([df_devset_nl.assign(mediaeval_set='dev'), df_testset_nl.assign(mediaeval_set='test')], ignore_index=True)\n",
    "\n",
    "df_joined_mediaeval_sets = pd.merge(df_joined, df_mediaeval_sets, how='left', on='name_sets')\n",
    "\n",
    "num_rows = len(df_joined_mediaeval_sets.index)\n",
    "\n",
    "dev_ids = []\n",
    "test_ids = []\n",
    "\n",
    "for index, row in tqdm(df_joined_mediaeval_sets.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    set = row['mediaeval_set']\n",
    "\n",
    "    if set == 'dev':\n",
    "        dev_ids.append(file_id)\n",
    "    elif set == 'test':\n",
    "        test_ids.append(file_id)\n",
    "\n",
    "print(len(dev_ids))\n",
    "print(len(test_ids))\n",
    "\n",
    "# Split devset into train and val sets\n",
    "df_devset_only = df_joined_mediaeval_sets[df_joined_mediaeval_sets['name_scores'].isin(dev_ids)]\n",
    "\n",
    "train_ids, val_ids = train_test_split(dev_ids, train_size=0.8, test_size=0.2, random_state=42, stratify=df_devset_only[['valenceClass', 'arousalClass']])\n",
    "\n",
    "torch.save(train_ids, 'preprocessed/data_splits/train_ids_mediaeval.pt')\n",
    "torch.save(test_ids, 'preprocessed/data_splits/test_ids_mediaeval.pt')\n",
    "torch.save(val_ids, 'preprocessed/data_splits/val_ids_mediaeval.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random data split\n",
    "\n",
    "Completely random data split with a ratio of 64% train, 16% val, and 20% test.\n",
    "\n",
    "Use the following parameter to use this data split when calling main.py: ```--dataset random```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "num_rows = len(df_joined.index)\n",
    "\n",
    "file_ids = []\n",
    "\n",
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    file_ids.append(file_id)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_ids, test_ids, val_ids = random_split(file_ids, [0.64, 0.2, 0.16], generator=generator)\n",
    "\n",
    "torch.save(train_ids, 'preprocessed/data_splits/train_ids_random.pt')\n",
    "torch.save(test_ids, 'preprocessed/data_splits/test_ids_random.pt')\n",
    "torch.save(val_ids, 'preprocessed/data_splits/val_ids_random.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified random data split\n",
    "\n",
    "Random data split stratified across valence and arousal class labels, with a ratio of 64% train, 16% val, and 20% test.\n",
    "\n",
    "Use the following parameter to use this data split when calling main.py: ```--dataset stratified```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "num_rows = len(df_joined)\n",
    "\n",
    "file_ids = []\n",
    "\n",
    "for index, row in tqdm(df_joined.iterrows(), total=num_rows):\n",
    "    file_id = row['name_scores']\n",
    "    file_ids.append(file_id)\n",
    "\n",
    "remaining_ids, test_ids = train_test_split(file_ids, train_size=0.8, test_size=0.2, random_state=42, stratify=df_joined[['valenceClass', 'arousalClass']])\n",
    "\n",
    "df_remaining = df_joined[df_joined['name_scores'].isin(remaining_ids)]\n",
    "\n",
    "train_ids, val_ids = train_test_split(remaining_ids, train_size=0.8, test_size=0.2, random_state=42, stratify=df_remaining[['valenceClass', 'arousalClass']])\n",
    "\n",
    "torch.save(train_ids, 'preprocessed/data_splits/train_ids_stratified.pt')\n",
    "torch.save(test_ids, 'preprocessed/data_splits/test_ids_stratified.pt')\n",
    "torch.save(val_ids, 'preprocessed/data_splits/val_ids_stratified.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
